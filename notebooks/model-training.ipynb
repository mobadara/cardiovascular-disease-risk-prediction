{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyNtBbEmn9bNbAffcT3MyEVr"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/muyiwaobadara/cardiovascular-disease-risk-prediction-training?scriptVersionId=254565407\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mobadara/cardiovascular-disease-risk-prediction/blob/main/notebooks/model-training.ipynb)\n[![Kaggle Notebook](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/new?source=https://github.com/mobadara/cardiovascular-disease-risk-prediction/blob/main/notebooks/model-training.ipynb)\n[![Python](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)\n\n# **Model Development - Cardiovascular Disease Risk Prediction**\n\n## **Introduction**\nThis notebook marks the beginning of the model development phase for our Cardiovascular Disease Risk Prediction project. Having thoroughly explored the dataset in the Exploratory Data Analysis (EDA) and enriched it with new features during Feature Engineering, we are now ready to train and compare various machine learning models.\n\n* **EDA Notebook:** [![Open In GitHub](https://img.shields.io/badge/View%20EDA%20Notebook-blue?logo=github)](https://github.com/mobadara/cardiovascular-disease-risk-prediction/blob/main/notebooks/exploratory-data-analysis.ipynb)\n* **Feature Engineering Notebook:** [![Open In GitHub](https://img.shields.io/badge/View%20FE%20Notebook-blue?logo=github)](https://github.com/mobadara/cardiovascular-disease-risk-prediction/blob/main/notebooks/feature-engineering.ipynb)\n\nIn this notebook, we will focus on building and evaluating several classification models to predict cardiovascular disease (`cardio`), including:\n\n* **Logistic Regression**\n* **Decision Tree / Random Forest**\n* **Gradient Boosting Machines (e.g., LightGBM, XGBoost)**\n* And potentially others like **Support Vector Machines (SVM)** or **K-Nearest Neighbors (KNN)**.\n\nWe will also implement essential preprocessing steps such as One-Hot Encoding and Standard Scaling, and carefully evaluate each model's performance using relevant metrics. Let's get started!","metadata":{"id":"YyxBzzXVy5Tg"}},{"cell_type":"markdown","source":"## **Notebook Setup**\n\nBefore diving into model development, we need to ensure all necessary libraries are imported and initial settings are configured. The following code cell will import the required Python libraries for data manipulation, numerical operations, machine learning model building, and visualization, and set up basic display options for pandas.","metadata":{"id":"-wytFxrW1MyC"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,\\\n                            roc_auc_score, confusion_matrix, classification_report, roc_curve,\\\n                            auc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\nprint('Setup Completed!')","metadata":{"id":"S40AUr1r0zG6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Data Loading**\n\nThe initial step in this model development phase is to load the dataset that has undergone the complete feature engineering process. This dataset, enriched with new features like BMI, age in years, and blood pressure categories, was the output of our previous feature engineering notebook and has been saved to the GitHub repository.\n\nThe following cell will load this prepared dataset directly from its raw URL on GitHub into a pandas DataFrame.\n","metadata":{"id":"ykXYNB270O94"}},{"cell_type":"code","source":"df = pd.read_csv('https://raw.githubusercontent.com/mobadara/cardiovascular-disease-risk-prediction/main/datasets/engineered.csv')\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"I-a72jE2ycAy","outputId":"2312260e-edfc-44de-e722-d22bdd78a77f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's make sure that the columns are in the right format.","metadata":{"id":"4rm1JP-Q6NKQ"}},{"cell_type":"code","source":"df.info()","metadata":{"id":"xFu10gTg6VcR","outputId":"f9834da7-f76f-4814-d5f2-2ca842f5fd7c","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we need to set the data type of each column to the appropriate format. This makes it easy to apply the appropriate transformation to the neccessary column.","metadata":{"id":"TWJIZ-UKqNGw"}},{"cell_type":"code","source":"df['gender'] = df['gender'].astype('category')\ndf['cholesterol'] = df['cholesterol'].astype('category')\ndf['gluc'] = df['gluc'].astype('category')\ndf['smoke'] = df['smoke'].astype('category')\ndf['alco'] = df['alco'].astype('category')\ndf['active'] = df['active'].astype('category')\ndf['age_group'] = df['age_group'].astype('category')\ndf['blood_pressure_category'] = df['blood_pressure_category'].astype('category')","metadata":{"id":"6VcVmZjDFfOk","trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.17Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"id":"D0KxoB3XFv1R","outputId":"44d2b096-80c1-4292-af8d-e6a523ce329c","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The data is loaded and transformed in to the appropriate format. We will now define a `preprocessing` pipeline. This pipeline enables us to apply **one-hot encoding** to categorical features, **standard scalling** to numerical columns and perform **feature selection**.\n\nThe trainig dataset does not contains missing value but we will handle it incase we encounter a missing datapoint in future predictions.\n\nWe are not applying log transformation since most of the numerical features are fairly normal and the **outliers** have been removed from the **feature engineering** section of the project.\n\nThe following code cell defines the preprocessing step.","metadata":{"id":"boCWdbjibZFi"}},{"cell_type":"code","source":"numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.drop('cardio')\ncategorical_features = df.select_dtypes(include=['category']).columns\nnum_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\ncat_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessing = ColumnTransformer(transformers=[\n        ('num', num_pipeline, numerical_features),\n        ('cat', cat_pipeline, categorical_features),\n    ],\n    remainder='passthrough'\n)\npreprocessing","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":190},"id":"neI-pyhCcYES","outputId":"8ff4afca-4a9a-4361-c2d2-c8398811e819","trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Train Set, Test Set**\nNow, we split the data into `train set` and `test set`. This will allow for efficient model evaluation during testing. We set aside 20% of the total instance for testing purpose.","metadata":{"id":"31NSyi1noOqQ"}},{"cell_type":"code","source":"target = 'cardio'\nX = df.drop(columns=[target])\ny = df[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,\n                                                    stratify=y)","metadata":{"id":"uSR8wgUhnxRx","trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Model Development**\nNow, that the data is splitted, it is time for actual training. We will train the classification with the algorithms outlined in the introduction section above.\n\nThe training process also involves feature selection, this is performed dynamically as part of the training.","metadata":{"id":"YBMAVQKRrFau"}},{"cell_type":"code","source":"selector = Pipeline(steps=[\n    ('preprocess', preprocessing),\n    ('feature_selection', SelectKBest(score_func=f_classif, k=10)) # k will be tuned\n])\nselector","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"id":"_elOo_GQonrM","outputId":"28dc2638-e663-41c2-ce09-31ac827991d1","trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.174Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The training process involves the following steps\n1. Initial model selection\n2. Cross Validation\n3. Hyperparameter Tuning\n4. Final Model Tuning","metadata":{"id":"fVF5aU41sQ74"}},{"cell_type":"markdown","source":"### **Initial Model Selection**\nIn this project, we will focus on building and evaluating several classification models to predict cardiovascular disease (`cardio`), including:\n\n* **Logistic Regression**\n* **Decision Tree / Random Forest**\n* **Gradient Boosting Machines (e.g., LightGBM, XGBoost)**\n* And potentially others like **Support Vector Machines (SVM)** or **K-Nearest Neighbors (KNN)**.","metadata":{"id":"kOzeS3GGx-Dp"}},{"cell_type":"markdown","source":"### **Cross Validataio**\nTo get an estimate of the performance of the selected models, we apply the **k-fold** cross validation on the model using the training data (`X_train`, `y_train`)","metadata":{"id":"e2m9Oh0PyK55"}},{"cell_type":"code","source":"mean_scores = []\n\nmodels = [\n    ['Logistic Regression', LogisticRegression()],\n    ['Decision Tree', DecisionTreeClassifier()],\n    ['Random Forest', RandomForestClassifier()],\n    ['Gradient Boost', GradientBoostingClassifier()],\n    ['XGBoost', XGBClassifier()],\n    ['Light GBM', LGBMClassifier()],\n    ['Support Vector Classifier', SVC()],\n    ['k-Nearest Neighbor', KNeighborsClassifier()]\n]\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nfor model in tqdm.tqdm(models, desc='Performing cross validation'):\n    pipeline = Pipeline(steps=[\n        ('selector', selector),\n        ('model', model[1])\n    ])\n    scores = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring='accuracy')\n    mean_scores.append(scores.mean())\nmodel_names = [model[0] for model in models]\nmean_cv_results = pd.DataFrame({'Model': model_names, 'Accuracy': mean_scores}).sort_values(by='Accuracy', ascending=False)\nmean_cv_results\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"opigXMctsO9n","outputId":"6cce2878-a5f4-46d5-fa2d-8707b1208049","trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.174Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In our estimate, it is seen that the models that produce high accuracies are enssemble models. We will take the five most accurate models and preform hyperparameter tuning on them. We will then compare the results.","metadata":{"id":"wcudWGm3QiGQ"}},{"cell_type":"code","source":"","metadata":{"id":"6Oe0KEJ-4rBI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Hyperparameter Tuning**\nNow that we have an estimate of all the performance of the seleted models, we will perform an hyperparameter tunning on the top six models, as seen in the dataframe in the above.\n\nWe will use the same range of selected features `k` for all the tuning process.","metadata":{"id":"TVz5iCMHWovB"}},{"cell_type":"code","source":"k_options = [5, 7, 10, 15, 'all'] # 'all' means keep all features after preprocessing\nall_best_results = {}","metadata":{"id":"u4RlkHkYCrNl","trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **`XGBoost` Tuning**","metadata":{"id":"IRnZbE1YYc5H"}},{"cell_type":"code","source":"print(\"\\n--- Tuning XGBoost ---\")\nxgb_pipeline = Pipeline(steps=[\n    ('selector', selector),\n    ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))\n])\n\nxgb_param_grid = {\n    'selector__feature_selection__k': k_options,\n    'classifier__n_estimators': [100, 200, 300],\n    'classifier__learning_rate': [0.01, 0.05, 0.1],\n    'classifier__max_depth': [3, 5, 7],\n    'classifier__subsample': [0.7, 0.9],\n    'classifier__colsample_bytree': [0.7, 0.9]\n}\n\nprint(\"XGBoost Parameter Grid Size:\", np.prod([len(v) for v in xgb_param_grid.values()]))\n\nxgb_grid_search = GridSearchCV(\n    xgb_pipeline,\n    xgb_param_grid,\n    cv=5,\n    scoring='roc_auc',\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(\"Starting XGBoost tuning...\")\n\nwith tqdm(total=1, desc=\"XGBoost Tuning Progress\") as pbar:\n    xgb_grid_search.fit(X_train, y_train)\n    pbar.update(1)\n\nprint(\"XGBoost Tuning Complete.\")\nall_best_results['XGBoost'] = {\n    \"score\": xgb_grid_search.best_score_,\n    \"params\": xgb_grid_search.best_params_\n}\n\nprint(f\"Best XGBoost Score: {all_best_results['XGBoost']['score']:.4f}\")\nprint(\"Best XGBoost Params:\", all_best_results['XGBoost']['params'])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W6BG55RVY7SR","outputId":"6f00c45a-3911-4277-e8ca-1cec71164e6d","trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **`LightGBM` Tuning**","metadata":{"id":"kaML8xtxgXK_"}},{"cell_type":"code","source":"print(\"\\n--- Tuning LightGBM ---\")\nlgbm_pipeline = Pipeline(steps=[\n    ('selector', selector),\n    ('classifier', LGBMClassifier(random_state=42))\n])\n\nlgbm_param_grid = {\n    'selector__feature_selection__k': k_options,\n    'classifier__n_estimators': [100, 200, 300],\n    'classifier__learning_rate': [0.01, 0.05, 0.1],\n    'classifier__max_depth': [5, 10, 15],\n    'classifier__num_leaves': [31, 63], # Specific to LightGBM\n    'classifier__subsample': [0.7, 0.9],\n    'classifier__colsample_bytree': [0.7, 0.9]\n}\n\nprint(\"LightGBM Parameter Grid Size:\", np.prod([len(v) for v in lgbm_param_grid.values()]))\n\nlgbm_grid_search = GridSearchCV(\n    lgbm_pipeline,\n    lgbm_param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(\"Starting LightGBM tuning...\")\nwith tqdm(total=1, desc=\"LightGBM Tuning Progress\") as pbar:\n    lgbm_grid_search.fit(X_train, y_train)\n    pbar.update(1)\n\nprint(\"LightGBM Tuning Complete.\")\nall_best_results['LightGBM'] = {\n    \"score\": lgbm_grid_search.best_score_,\n    \"params\": lgbm_grid_search.best_params_\n}\nprint(f\"Best LightGBM Score: {all_best_results['LightGBM']['score']:.4f}\")\nprint(\"Best LightGBM Params:\", all_best_results['LightGBM']['params'])","metadata":{"id":"1tfzRYUDgYOs","trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.176Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Gradient Boosting**","metadata":{"id":"9SMHKmlMhIOw"}},{"cell_type":"code","source":"print(\"\\n--- Tuning Gradient Boosting Classifier ---\")\ngb_pipeline = Pipeline(steps=[\n    ('selector', selector),\n    ('classifier', GradientBoostingClassifier(random_state=42))\n])\n\ngb_param_grid = {\n    'selector__feature_selection__k': k_options,\n    'classifier__n_estimators': [100, 200, 300],\n    'classifier__learning_rate': [0.01, 0.05, 0.1],\n    'classifier__max_depth': [3, 5, 7],\n    'classifier__subsample': [0.7, 0.9],\n    'classifier__max_features': [0.7, 0.9, 'sqrt'] # Specific to GBC\n}\n\nprint(\"Gradient Boosting Parameter Grid Size:\", np.prod([len(v) for v in gb_param_grid.values()]))\n\ngb_grid_search = GridSearchCV(\n    gb_pipeline,\n    gb_param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(\"Starting Gradient Boosting Classifier tuning...\")\nwith tqdm(total=1, desc=\"GB Classifier Tuning Progress\") as pbar:\n    gb_grid_search.fit(X_train, y_train)\n    pbar.update(1)\n\nprint(\"Gradient Boosting Classifier Tuning Complete.\")\nall_best_results['Gradient Boosting'] = {\n    \"score\": gb_grid_search.best_score_,\n    \"params\": gb_grid_search.best_params_\n}\nprint(f\"Best GB Classifier Score: {all_best_results['Gradient Boosting']['score']:.4f}\")\nprint(\"Best GB Classifier Params:\", all_best_results['Gradient Boosting']['params'])","metadata":{"id":"npBkARWXhNMr","trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.177Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Support Vector Classifier**\n\nFor SVC, especially with larger datasets, `RandomizedSearchCV` might be more practical","metadata":{"id":"-iaHh9e5hXyH"}},{"cell_type":"code","source":"print(\"\\n--- Tuning Support Vector Classifier (SVC) ---\")\nsvc_pipeline = Pipeline(steps=[\n    ('selector', selector),\n    ('classifier', SVC(random_state=42, probability=True)) # probability=True for ROC AUC if needed\n])\n\n# SVC can be computationally expensive, so a smaller grid or RandomizedSearchCV is often better\nsvc_param_grid = {\n    'selector__feature_selection__k': [5, 10, 'all'], # Reduced k options for faster tuning\n    'classifier__C': [0.1, 1, 10], # Regularization parameter\n    'classifier__kernel': ['linear', 'rbf'], # Kernel type\n    'classifier__gamma': ['scale', 'auto'] # Kernel coefficient for 'rbf'\n}\n\nprint(\"SVC Parameter Grid Size:\", np.prod([len(v) for v in svc_param_grid.values()]))\n\nsvc_grid_search = GridSearchCV( # Using GridSearchCV for demonstration\n    svc_pipeline,\n    svc_param_grid,\n    cv=3, # Reduced CV folds for SVC due to higher computational cost\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1,\n    # n_iter=20 # For RandomizedSearchCV, specify number of iterations\n)\n\nprint(\"Starting SVC tuning...\")\nwith tqdm(total=1, desc=\"SVC Tuning Progress\") as pbar:\n    svc_grid_search.fit(X_train, y_train)\n    pbar.update(1)\n\nprint(\"SVC Tuning Complete.\")\nall_best_results['Support Vector Classifier'] = {\n    \"score\": svc_grid_search.best_score_,\n    \"params\": svc_grid_search.best_params_\n}\nprint(f\"Best SVC Score: {all_best_results['Support Vector Classifier']['score']:.4f}\")","metadata":{"id":"2mrufGVohejx","trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.177Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Logistic Regression**","metadata":{"id":"9bvBrtA9i46a"}},{"cell_type":"code","source":"print(\"\\n--- Tuning Logistic Regression ---\")\nlr_pipeline = Pipeline(steps=[\n    ('selector', selector),\n    ('classifier', LogisticRegression(solver='liblinear', random_state=42)) # liblinear is good for small datasets\n])\n\nlr_param_grid = {\n    'selector__feature_selection__k': k_options,\n    'classifier__C': [0.01, 0.1, 1, 10, 100], # Inverse of regularization strength\n    'classifier__penalty': ['l1', 'l2'] # Regularization type\n}\n\nprint(\"Logistic Regression Parameter Grid Size:\", np.prod([len(v) for v in lr_param_grid.values()]))\n\nlr_grid_search = GridSearchCV(\n    lr_pipeline,\n    lr_param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(\"Starting Logistic Regression tuning...\")\nwith tqdm(total=1, desc=\"Logistic Regression Tuning Progress\") as pbar:\n    lr_grid_search.fit(X_train, y_train)\n    pbar.update(1)\n\nprint(\"Logistic Regression Tuning Complete.\")\nall_best_results['Logistic Regression'] = {\n    \"score\": lr_grid_search.best_score_,\n    \"params\": lr_grid_search.best_params_\n}\nprint(f\"Best Logistic Regression Score: {all_best_results['Logistic Regression']['score']:.4f}\")\nprint(\"Best Logistic Regression Params:\", all_best_results['Logistic Regression']['params'])","metadata":{"id":"ghjhSUUZjAUq","trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Random Forest**","metadata":{"id":"YyfrhXyRjK0_"}},{"cell_type":"code","source":"print(\"\\n--- Tuning Random Forest ---\")\nrf_pipeline = Pipeline(steps=[\n    ('selector', selector),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\nrf_param_grid = {\n    'selector__feature_selection__k': k_options,\n    'classifier__n_estimators': [100, 200, 300],\n    'classifier__max_depth': [None, 10, 20], # None means nodes are expanded until all leaves are pure\n    'classifier__min_samples_split': [2, 5],\n    'classifier__min_samples_leaf': [1, 2]\n}\n\nprint(\"Random Forest Parameter Grid Size:\", np.prod([len(v) for v in rf_param_grid.values()]))\n\nrf_grid_search = GridSearchCV(\n    rf_pipeline,\n    rf_param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(\"Starting Random Forest tuning...\")\nwith tqdm(total=1, desc=\"Random Forest Tuning Progress\") as pbar:\n    rf_grid_search.fit(X_train, y_train)\n    pbar.update(1)\n\nprint(\"Random Forest Tuning Complete.\")\nall_best_results['Random Forest'] = {\n    \"score\": rf_grid_search.best_score_,\n    \"params\": rf_grid_search.best_params_\n}\nprint(f\"Best Random Forest Score: {all_best_results['Random Forest']['score']:.4f}\")\nprint(\"Best Random Forest Params:\", all_best_results['Random Forest']['params'])\nprint(\"-\" * 50)\n","metadata":{"id":"J2OfFfzIjOEA","trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sorted_results = sorted(all_best_results.items(), key=lambda item: item[1]['score'], reverse=True)\n\nfor model_name, result in sorted_results:\n    print(f\"Model: {model_name}\")\n    print(f\"  Best CV Score: {result['score']:.4f}\")\n    print(f\"  Best Parameters: {result['params']}\")\n    print(\"-\" * 30)","metadata":{"id":"hQTnAwtuW-Ch","trusted":true,"execution":{"execution_failed":"2025-08-06T11:16:52.179Z"}},"outputs":[],"execution_count":null}]}